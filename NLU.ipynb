{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use machine learning to turn natural language into structured data using spaCy, scikit-learn, and rasa NLU.\n",
    "# Understanding intents and entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greet': re.compile('hello|hi|hey'), 'goodbye': re.compile('bye|farewell'), 'thankyou': re.compile('thank|tnx')}\n"
     ]
    }
   ],
   "source": [
    "# Intent classification with regex I, looking for keywords\n",
    "import re\n",
    "\n",
    "keywords = {'greet': ['hello', 'hi', 'hey'], \n",
    "            'goodbye': ['bye', 'farewell'], \n",
    "            'thankyou': ['thank', 'tnx']}\n",
    "\n",
    "responses = {'default': 'default message',\n",
    "            'goodbye': 'goodbye for now',\n",
    "            'greet': 'Hello you! :)',\n",
    "            'thankyou': 'you are very welcome'}\n",
    "\n",
    "# Define a dictionary of patterns\n",
    "patterns = {}\n",
    "\n",
    "# Iterate over the keywords dictionary\n",
    "for intent, keys in keywords.items():\n",
    "    # Create regular expressions and compile them into pattern objects\n",
    "    patterns[intent] = re.compile('|'.join(keys))\n",
    "    \n",
    "# Print the patterns\n",
    "print(patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the intent of a message\n",
    "def match_intent(message):\n",
    "    matched_intent = None\n",
    "    for intent, pattern in patterns.items():\n",
    "        # Check if the pattern occurs in the message \n",
    "        if pattern.search(message):\n",
    "            matched_intent = intent\n",
    "    return matched_intent\n",
    "\n",
    "# Define a respond function\n",
    "def respond(message):\n",
    "    # Call the match_intent function\n",
    "    intent = match_intent(message)\n",
    "    # Fall back to the default response\n",
    "    key = \"default\"\n",
    "    if intent in responses:\n",
    "        key = intent\n",
    "    return responses[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : my name is David Copperfield\n",
      "BOT : Hello, David Copperfield!\n",
      "USER : call me Sia\n",
      "BOT : Hi there!\n",
      "USER : People call me Cassandra\n",
      "BOT : Hi there!\n"
     ]
    }
   ],
   "source": [
    "# Entity extraction with regex\n",
    "\n",
    "bot_template = \"BOT : {0}\"\n",
    "user_template = \"USER : {0}\"\n",
    "\n",
    "# Define a function that sends a message to the bot: send_message\n",
    "def send_message(message):\n",
    "    # Print user_template including the user_message\n",
    "    print(user_template.format(message))\n",
    "    # Get the bot's response to the message\n",
    "    response = respond(message)\n",
    "    # Print the bot template including the bot's response.\n",
    "    print(bot_template.format(response))\n",
    "\n",
    "# Define find_name()\n",
    "def find_name(message):\n",
    "    name = None\n",
    "    # Create a pattern for checking if the keywords occur\n",
    "    name_keyword = re.compile('name|call(ed)')\n",
    "    # Create a pattern for finding capitalized words\n",
    "    name_pattern = re.compile('[A-Z]{1}[a-z]*')\n",
    "    if name_keyword.search(message):\n",
    "        # Get the matching words in the string\n",
    "        name_words = name_pattern.findall(message)\n",
    "        if len(name_words) > 0:\n",
    "            # Return the name if the keywords are present\n",
    "            name = ' '.join(name_words)\n",
    "    return name\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Find the name\n",
    "    name = find_name(message)\n",
    "    if name is None:\n",
    "        return \"Hi there!\"\n",
    "    else:\n",
    "        return \"Hello, {0}!\".format(name)\n",
    "\n",
    "# Send messages\n",
    "send_message(\"my name is David Copperfield\")\n",
    "send_message(\"call me Sia\")\n",
    "send_message(\"People call me Cassandra\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Vectors\n",
    "# Apply ML to text\n",
    "# Vectors can be units of characters, words or sentences\n",
    "# word vectors try to represent meaning of words\n",
    "# words which appear in similar context have similar vectors\n",
    "# word2vec\n",
    "# .similarity() Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "sentences = [' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', ' what flights are available from pittsburgh to baltimore on thursday morning', ' what is the arrival time in san francisco for the 755 am flight leaving washington', ' cheapest airfare from tacoma to orlando', ' round trip fares from pittsburgh to philadelphia under 1000 dollars', ' i need a flight tomorrow from columbus to minneapolis', ' what kind of aircraft is used on a flight from cleveland to dallas', ' show me the flights from pittsburgh to los angeles on thursday', ' all flights from boston to washington', ' what kind of ground transportation is available in denver', ' show me the flights from dallas to san francisco', ' show me the flights from san diego to newark by way of houston', ' what is the cheapest flight from boston to bwi', ' all flights to baltimore after 6 pm', ' show me the first class fares from boston to denver', ' show me the ground transportation in denver', ' all flights from denver to pittsburgh leaving after 6 pm and before 7 pm', ' i need information on flights for tuesday leaving baltimore for dallas dallas to boston and boston to baltimore', ' please give me the flights from boston to pittsburgh on thursday of next week', ' i would like to fly from denver to pittsburgh on united airlines', ' show me the flights from san diego to newark', ' please list all first class flights on united from denver to baltimore', ' what kinds of planes are used by american airlines', \" i'd like to have some information on a ticket from denver to pittsburgh and atlanta\", \" i'd like to book a flight from atlanta to denver\", ' which airline serves denver pittsburgh and atlanta', \" show me all flights from boston to pittsburgh on wednesday of next week which leave boston after 2 o'clock pm\", ' atlanta ground transportation', ' i also need service from dallas to boston arriving by noon', ' show me the cheapest round trip fare from baltimore to dallas']\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_DIM: 300\n"
     ]
    }
   ],
   "source": [
    "# Load the spacy model: nlp\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Calculate the length of sentences\n",
    "n_sentences = len(sentences)\n",
    "\n",
    "# Calculate the dimensionality of nlp\n",
    "embedding_dim = nlp.vocab.vectors_length\n",
    "print('VOCAB_DIM:', embedding_dim)\n",
    "\n",
    "# Initialize the array with zeros: X\n",
    "X = np.zeros((n_sentences, embedding_dim))\n",
    "\n",
    "# Iterate over the sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Pass each each sentence to the nlp object to create a document\n",
    "    doc = nlp(sentence)\n",
    "    # Save the document's .vector attribute to the corresponding row in X\n",
    "    X[idx, :] = doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intents & Classification\n",
    "# use supervised learning, fit classifier, nearest neighbor in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4978 entries, 0 to 4977\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype   \n",
      "---  ------   --------------  -----   \n",
      " 0   label    4978 non-null   category\n",
      " 1   message  4978 non-null   string  \n",
      "dtypes: category(1), string(1)\n",
      "memory usage: 44.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Intent classification with sklearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('Files/atis_intents.csv')\n",
    "df.label = df.label.astype('category')\n",
    "df.message = df.label.astype('string')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (3335, 300)\n",
      "X_test (1643, 300)\n",
      "y_train (3335,)\n",
      "y_test (1643,)\n"
     ]
    }
   ],
   "source": [
    "def build_features(sentences):\n",
    "    # Calculate the length of sentences\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    # Calculate the dimensionality of nlp\n",
    "    embedding_dim = nlp.vocab.vectors_length\n",
    "\n",
    "    # Initialize the array with zeros: X\n",
    "    X = np.zeros((n_sentences, embedding_dim))\n",
    "\n",
    "    # Iterate over the sentences\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        # Pass each each sentence to the nlp object to create a document\n",
    "        doc = nlp(sentence)\n",
    "        # Save the document's .vector attribute to the corresponding row in X\n",
    "        X[idx, :] = doc.vector\n",
    "\n",
    "    return X\n",
    "\n",
    "X = build_features(df['message'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.label, test_size=0.33, random_state=42)\n",
    "\n",
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                       atis_abbreviation       0.00      0.00      0.00        38\n",
      "                           atis_aircraft       0.00      0.00      0.00        22\n",
      "atis_aircraft#atis_flight#atis_flight_no       0.00      0.00      0.00         1\n",
      "                            atis_airfare       0.00      0.00      0.00       141\n",
      "                            atis_airline       0.00      0.00      0.00        49\n",
      "                            atis_airport       0.00      0.00      0.00         4\n",
      "                           atis_capacity       0.00      0.00      0.00         4\n",
      "                               atis_city       0.00      0.00      0.00         8\n",
      "                           atis_distance       0.00      0.00      0.00         4\n",
      "                             atis_flight       0.76      1.00      0.86      1242\n",
      "                atis_flight#atis_airfare       0.00      0.00      0.00         8\n",
      "                          atis_flight_no       0.00      0.00      0.00         6\n",
      "                        atis_flight_time       0.00      0.00      0.00        16\n",
      "                        atis_ground_fare       0.00      0.00      0.00         3\n",
      "                     atis_ground_service       0.00      0.00      0.00        72\n",
      "                               atis_meal       0.00      0.00      0.00         2\n",
      "                           atis_quantity       0.00      0.00      0.00        21\n",
      "                        atis_restriction       0.00      0.00      0.00         2\n",
      "\n",
      "                                accuracy                           0.76      1643\n",
      "                               macro avg       0.04      0.06      0.05      1643\n",
      "                            weighted avg       0.57      0.76      0.65      1643\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sia/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/Sia/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/Sia/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a support vector classifier\n",
    "clf = SVC(C=1)\n",
    "\n",
    "# Fit the classifier using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Count the number of correct predictions\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Extraction\n",
    "# Using spaCy's entity recognizer\n",
    "# spaCy's built-in entity recognizer to extract names, dates, and organizations from search queries.\n",
    "# define a function called extract_entities(), which takes in a single argument message and returns a dictionary with the included entity types as keys, and the extracted entities as values. The included entity types are contained in a list called include_entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': '2010', 'ORG': 'Google', 'PERSON': 'Mary'}\n",
      "{'DATE': '2022', 'ORG': 'UPC', 'PERSON': 'Babak'}\n"
     ]
    }
   ],
   "source": [
    "# Define included_entities\n",
    "include_entities = ['DATE', 'ORG', 'PERSON']\n",
    "\n",
    "# Define extract_entities()\n",
    "def extract_entities(message):\n",
    "    # Create a dict to hold the entities\n",
    "    ents = dict.fromkeys(include_entities)\n",
    "    # Create a spacy document\n",
    "    doc = nlp(message)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ents:\n",
    "            # Save interesting entities\n",
    "            ents[ent.label_] = ent.text\n",
    "    return ents\n",
    "\n",
    "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
    "print(extract_entities('Babak who graduated from UPC in 2022'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning roles using spaCy's parser\n",
    "# Create the document\n",
    "doc = nlp(\"let's see that jacket in red and some blue jeans\")\n",
    "\n",
    "# Iterate over parents in parse tree until an item entity is found\n",
    "def find_parent_item(word):\n",
    "    # Iterate over the word's ancestors\n",
    "    for parent in word.ancestors:\n",
    "        # Check for an \"item\" entity\n",
    "        if entity_type(parent) == \"item\":\n",
    "            return parent.text\n",
    "    return None\n",
    "\n",
    "# For all color entities, find their parent item\n",
    "def assign_colors(doc):\n",
    "    # Iterate over the document\n",
    "    for word in doc:\n",
    "        # Check for \"color\" entities\n",
    "        if entity_type(word) == \"color\":\n",
    "            # Find the parent\n",
    "            item = find_parent_item(word)\n",
    "            print(\"item: {0} has color : {1}\".format(item, word))\n",
    "\n",
    "# Assign the colors\n",
    "assign_colors(doc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90d056296a45556521e929369b7ac4560abf6ad78fcd7dc94e39c36823bee644"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
